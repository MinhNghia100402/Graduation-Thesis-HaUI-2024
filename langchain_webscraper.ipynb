{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "urls = [\"https://fit.haui.edu.vn/vn\"]\n",
    "loader = AsyncHtmlLoader(urls)\n",
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_extraction_chain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# os.environ['GOOGLE_API_KEY'] =os.environ[\"GEMINI_API_KEY\"]\n",
    "# print(os.environ.get('GOOGLE_API_KEY'))\n",
    "\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"news_article_title\": {\"type\": \"string\"},\n",
    "        \"news_article_summary\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"news_article_title\", \"news_article_summary\"],\n",
    "}\n",
    "\n",
    "\n",
    "def extract(content: str, schema: dict):\n",
    "    return create_extraction_chain(schema=schema, llm=model).run(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "model_name=\"dangvantuan/vietnamese-embedding\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddingsModel = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "model_name=\"dangvantuan/vietnamese-embedding\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddingsModel = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "def scrape_with_playwright(urls, schema):\n",
    "    loader = AsyncHtmlLoader(urls)\n",
    "    docs = loader.load()\n",
    "    bs_transformer = BeautifulSoupTransformer()\n",
    "    docs_transformed = bs_transformer.transform_documents(\n",
    "        docs, tags_to_extract=['span','p','li','']\n",
    "    )\n",
    "\n",
    "    text_splitter = SemanticChunker(embeddingsModel,breakpoint_threshold_type=\"gradient\")\n",
    "    \n",
    "    splits = text_splitter.split_documents(docs_transformed)\n",
    "    print(splits)\n",
    "\n",
    "    # Process the first split\n",
    "    extracted_content = extract(content=splits[0].page_content,schema=schema)\n",
    "    pprint.pprint(extracted_content)\n",
    "    return extracted_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"news_article_title\": {\"type\": \"string\"},\n",
    "        \"news_article_summary\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"news_article_title\", \"news_article_summary\"],\n",
    "}\n",
    "\n",
    "# with open('link_result.txt','r') as link_result:\n",
    "#     urls = link_result.readlines()\n",
    "# print(urls)\n",
    "urls = [\"https://fit.haui.edu.vn/vn\"]\n",
    "extracted_content = scrape_with_playwright(urls, schema=schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
